{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('/Users/kleiofragkedaki/Desktop/thesis/master-thesis-2023-reinforcement-learning-in-grids/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Tuple, Union, TypeVar\n",
    "from gym import Env\n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ObsType = TypeVar(\"ObsType\")\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.graph.evrp_network import EVRPNetwork\n",
    "from src.graph.evrp_graph import EVRPGraph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_input_dim: int,\n",
    "        embedding_dim: int = 128,\n",
    "        hidden_dim: int = 512,\n",
    "        num_attention_layers: int = 3,\n",
    "        num_heads: int = 8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initalizes the GraphEncoder\n",
    "\n",
    "        Args:\n",
    "            node_input_dim (int): Feature Dimension of input nodes\n",
    "            embedding_dim (int): Number of dimensions in the embedding space.\n",
    "            hidden_dim (int): Number of neurons of the hidden layer of the fcl.\n",
    "            num_attention_layers (int): Number of attention layers.\n",
    "            num_heads (int): Number of heads in each attention layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # initial embeds ff layer for each nodes type\n",
    "        self.node_embed = nn.Linear(node_input_dim, embedding_dim)\n",
    "\n",
    "        self.attention_layers = nn.ModuleList(\n",
    "            [\n",
    "                MultiHeadAttentionLayer(\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    hidden_dim=hidden_dim,\n",
    "                    num_heads=num_heads,\n",
    "                )\n",
    "                for _ in range(num_attention_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the node embedding for each node\n",
    "        in each graph.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Shape (num_graphs, num_nodes, num_features)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Returns the embedding of each node in each graph.\n",
    "                Shape (num_graphs, num_nodes, embedding_dim).\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.node_embed(x)\n",
    "        for layer in self.attention_layers:\n",
    "            out = layer(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Coverts inputs of (N, L, C) to (N*L, C)\n",
    "    s.t we can apply BatchNorm for the\n",
    "    features C.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.norm = nn.BatchNorm1d(feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.size()\n",
    "        return self.norm(x.view(-1, shape[-1])).view(*shape)\n",
    "\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "\n",
    "    def __init__(self, module):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input + self.module(input)\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, hidden_dim: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        MultiHeadAttentionLayer with skip connection, batch\n",
    "        normalization and a fully connected network.\n",
    "\n",
    "        Args:\n",
    "            embedding_dim (int): Number of dimensions in the embedding space.\n",
    "            hidden_dim (int): Number of neurons of the hidden layer of the fcl.\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention_layer = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_dim, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.bn1 = BatchNorm(embedding_dim)\n",
    "        self.bn2 = BatchNorm(embedding_dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embedding_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape\n",
    "                (num_graph, num_nodes, num_features)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of shape\n",
    "                (num_graph, num_nodes, embedding_dim)\n",
    "        \"\"\"\n",
    "        out = self.bn1(x + self.attention_layer(x, x, x)[0]) # Skip connection and normalize\n",
    "        out = self.bn2(out + self.ff(out)) # Skip connection and normalize\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder Class to generate node prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim: int = 128,\n",
    "        num_heads: int = 8,\n",
    "        v_dim: int = 128,\n",
    "        k_dim: int = 128,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            emb_dim (int): Dimension of the input embedding.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            v_dim (int): Dimension of attention value.\n",
    "            k_dim (int): Dimension of attention key.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self._first_node = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
    "        self._last_node = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=3 * emb_dim,\n",
    "            num_heads=num_heads,\n",
    "            kdim=k_dim,\n",
    "            vdim=v_dim,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self._kp = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self._att_output = nn.Linear(emb_dim * 3, emb_dim, bias=False)\n",
    "\n",
    "        # project in context of [graph_emb, ]\n",
    "        self._context_proj = nn.Linear(emb_dim * 2 + 1, emb_dim * 3, bias=False)\n",
    "\n",
    "        self.first_ = None\n",
    "        self.last_ = None\n",
    "        self.first_step = True\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_embs: torch.Tensor,\n",
    "        mask: torch.Tensor = None,\n",
    "        load: torch.Tensor = None, # TODO\n",
    "        C: int = 10,\n",
    "        rollout: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward method of the Decoder\n",
    "\n",
    "        Args:\n",
    "            node_embs (torch.Tensor): Node embeddings with shape (batch_size, num_nodes, emb_dim)\n",
    "            mask (torch.Tensor, optional): Node mask with shape (batch_size, num_nodes). Defaults to None.\n",
    "            load (torch.Tensor, optional): Load of the vehicle with shape (batch_size, 1). Defaults to None.\n",
    "            C (int, optional): Hyperparameter to regularize logit calculation. Defaults to 10.\n",
    "            rollout (bool, optional): Determines if prediction is sampled or maxed. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Node prediction for each graph with shape (batch_size, 1)\n",
    "            torch.Tensor: Log probabilities\n",
    "        \"\"\"\n",
    "        batch_size, _, emb_dim = node_embs.shape\n",
    "\n",
    "        graph_emb = torch.mean(\n",
    "            node_embs, axis=1, keepdims=True\n",
    "        )  # shape (batch, 1, emb)\n",
    "\n",
    "        if self.first_ is None:\n",
    "            self.first_ = self._first_node.repeat(batch_size, 1, 1)\n",
    "            self.last_ = self._last_node.repeat(batch_size, 1, 1)\n",
    "\n",
    "        k = self._kp(node_embs)\n",
    "\n",
    "        # Create context with first, last node and graph embedding.\n",
    "        # Where last is the node from last decoding step.\n",
    "        if load is None:\n",
    "            context = torch.cat([graph_emb, self.first_, self.last_], -1)\n",
    "        else:\n",
    "            context = torch.cat([graph_emb, self.last_, load[:, None, None]], -1)\n",
    "            context = self._context_proj(context)\n",
    "\n",
    "        attn_mask = mask.repeat(self.num_heads, 1).unsqueeze(1)\n",
    "        q, _ = self.attention(context, node_embs, node_embs, attn_mask=attn_mask)\n",
    "        q = self._att_output(q)\n",
    "\n",
    "        u = torch.tanh(q.bmm(k.transpose(-2, -1)) / emb_dim ** 0.5) * C\n",
    "        u = u.masked_fill(mask.unsqueeze(1).bool(), float(\"-inf\"))\n",
    "\n",
    "        log_prob = torch.zeros(size=(batch_size,))\n",
    "        nn_idx = None\n",
    "        if rollout:\n",
    "            nn_idx = u.argmax(-1)\n",
    "        else:\n",
    "            m = Categorical(logits=u)\n",
    "            nn_idx = m.sample()\n",
    "            log_prob = m.log_prob(nn_idx)\n",
    "        temp = nn_idx.unsqueeze(-1).repeat(1, 1, emb_dim)\n",
    "        self.last_ = torch.gather(node_embs, 1, temp)\n",
    "\n",
    "        if self.first_step:\n",
    "            self.first_ = self.last_\n",
    "            self.first_step = False\n",
    "\n",
    "        return nn_idx, log_prob\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets first and last node.\n",
    "        Must be called before starting a new game.\n",
    "        \"\"\"\n",
    "        self.first_ = None\n",
    "        self.last_ = None\n",
    "        self.first_step = True\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVRPEnv(Env):\n",
    "    \"\"\"\n",
    "    EVRPEnv implements the Electric Vehicle Routing Problem\n",
    "    a special variant of the vehicle routing problem that decouples the trailer and the truck.\n",
    "\n",
    "    State: Shape (batch_size, num_nodes, 4) The third\n",
    "        dimension is structured as follows:\n",
    "        [x_coord, y_coord, num_chargers, available_chargers, trailer, truck, visitable]\n",
    "\n",
    "    Actions: Depends on the number of nodes, trailers and trucks in every graph.\n",
    "        Should contain the truck numbers, the trailer numbers and the node numbers \n",
    "        to visit next for each graph. Shape (batch_size, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\", \"rgb_array\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_nodes: int = 4,\n",
    "        batch_size: int = 128,\n",
    "        num_draw: int = 6,\n",
    "        seed: int = 69,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_nodes (int, optional): Number of nodes in each generated graph. Defaults to 4.\n",
    "            batch_size (int, optional): Number of graphs to generate. Defaults to 128.\n",
    "            num_draw (int, optional): When calling the render num_draw graphs will be rendered. \n",
    "                Defaults to 6.\n",
    "            seed (int, optional): Seed of the environment. Defaults to 69.\n",
    "            video_save_path (str, optional): When set a video of the interactions with the \n",
    "                environment is saved at the set location. Defaults to None.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            num_draw <= batch_size\n",
    "        ), \"Num_draw needs to be equal or lower than the number of generated graphs.\"\n",
    "\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.num_nodes = num_nodes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # init video recorder\n",
    "        self.draw_idxs = np.random.choice(batch_size, num_draw, replace=False)\n",
    "        self.video_save_path = None\n",
    "\n",
    "        self.generate_graphs()\n",
    "\n",
    "    def step(self, actions: np.ndarray) -> Tuple[ObsType, float, bool, dict]:\n",
    "        \"\"\"\n",
    "        Run the environment one timestep. It's the users responsiblity to\n",
    "        call reset() when the end of the episode has been reached. Accepts\n",
    "        an action and return a tuple of (observation, reward, done, info)\n",
    "\n",
    "        Args:\n",
    "            actions (nd.ndarray): Which node to visit for each graph.\n",
    "                Shape of actions is (batch_size, 1).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[ObsType, float, bool, dict]: Tuple of the observation,\n",
    "                reward, done and info. The observation is within\n",
    "                self.observation_space. The reward is for the previous action.\n",
    "                If done equals True then the episode is over. Stepping through\n",
    "                environment while done returns undefined results. Info contains\n",
    "                may contain additions info in terms of metrics, state variables\n",
    "                and such.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            actions.shape[0] == self.batch_size\n",
    "        ), \"Number of actions need to equal the number of generated graphs.\"\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "        # visit each next node\n",
    "        self.visited[np.arange(len(actions)), actions.T] = 1\n",
    "        traversed_edges = np.hstack([self.current_location, actions]).astype(int)\n",
    "        self.sampler.visit_edges(traversed_edges)\n",
    "\n",
    "        self.current_location = np.array(actions)\n",
    "\n",
    "        if self.video_save_path is not None:\n",
    "            self.vid.capture_frame()\n",
    "\n",
    "        done = self.is_done()\n",
    "        return (\n",
    "            self.get_state(),\n",
    "            -self.sampler.get_distances(traversed_edges),\n",
    "            done,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    def is_done(self):\n",
    "        return np.all(self.visited == 1)\n",
    "\n",
    "    def get_state(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Getter for the current environment state\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Shape (num_graph, num_nodes, 4)\n",
    "            where the third dimension consists of the\n",
    "            x, y coordinates, if the node is a depot,\n",
    "            and if it has been visted yet.\n",
    "        \"\"\"\n",
    "\n",
    "        # generate state (depots not yet set)\n",
    "        state = np.dstack(\n",
    "            [\n",
    "                self.sampler.get_graph_positions(),\n",
    "                np.zeros((self.batch_size, self.num_nodes)),\n",
    "                self.generate_mask(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # set depots in state to 1\n",
    "        state[np.arange(len(state)), self.depots.T, 2] = 1\n",
    "\n",
    "        return state\n",
    "\n",
    "    def generate_mask(self):\n",
    "        \"\"\"\n",
    "        Generates a mask of where the nodes marked as 1 cannot \n",
    "        be visited in the next step according to the env dynamic.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Returns mask for each (un)visitable node \n",
    "                in each graph. Shape (batch_size, num_nodes)\n",
    "        \"\"\"\n",
    "        # disallow staying on a depot\n",
    "        depot_graphs_idxs = np.where(self.current_location == self.depots)[0]\n",
    "        self.visited[depot_graphs_idxs, self.depots[depot_graphs_idxs].squeeze()] = 1\n",
    "\n",
    "        # allow staying on a depot if the graph is solved.\n",
    "        done_graphs = np.where(np.all(self.visited, axis=1) == True)[0]\n",
    "        self.visited[done_graphs, self.depots[done_graphs].squeeze()] = 0\n",
    "\n",
    "        return self.visited\n",
    "\n",
    "    def reset(self) -> Union[ObsType, Tuple[ObsType, dict]]:\n",
    "        \"\"\"\n",
    "        Resets the environment. \n",
    "\n",
    "        Returns:\n",
    "            Union[ObsType, Tuple[ObsType, dict]]: State of the environment.\n",
    "        \"\"\"\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.generate_graphs()\n",
    "        return self.get_state()\n",
    "\n",
    "    def generate_graphs(self):\n",
    "        \"\"\"\n",
    "        Generates a VRPNetwork of batch_size graphs with num_nodes\n",
    "        each. Resets the visited nodes to 0.\n",
    "        \"\"\"\n",
    "        self.visited = np.zeros(shape=(self.batch_size, self.num_nodes))\n",
    "        self.sampler = EVRPNetwork(\n",
    "            num_graphs=self.batch_size, num_nodes=self.num_nodes\n",
    "        )\n",
    "\n",
    "        # set current location to the depots\n",
    "        self.depots = self.sampler.get_depots()\n",
    "        self.current_location = self.depots\n",
    "\n",
    "    def render(self, mode: str = \"human\"):\n",
    "        \"\"\"\n",
    "        Visualize one step in the env. Since its batched \n",
    "        this methods renders n random graphs from the batch.\n",
    "        \"\"\"\n",
    "        return self.sampler.draw(self.draw_idxs)\n",
    "\n",
    "    def enable_video_capturing(self, video_save_path: str):\n",
    "        self.video_save_path = video_save_path\n",
    "        if self.video_save_path is not None:\n",
    "            self.vid = VideoRecorder(self, self.video_save_path)\n",
    "            self.vid.frames_per_sec = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_dim: int,\n",
    "        emb_dim: int,\n",
    "        hidden_dim: int,\n",
    "        num_attention_layers: int,\n",
    "        num_heads: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The TSPModel is used in companionship with the TSPEnv\n",
    "        to solve the capacited vehicle routing problem.\n",
    "\n",
    "        Args:\n",
    "            depot_dim (int): Input dimension of a depot node.\n",
    "            node_dim (int): Input dimension of a regular graph node.\n",
    "            emb_dim (int): Size of a vector in the embedding space.\n",
    "            hidden_dim (int): Dimension of the hidden layers of the \n",
    "                ff-network layers within the graph-encoder.\n",
    "            num_attention_layers (int): Number of attention layers \n",
    "                for both the graph-encoder and -decoder.\n",
    "            num_heads (int): Number of attention heads in each \n",
    "                MultiHeadAttentionLayer for both the graph-encoder and -decoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.encoder = GraphEncoder(\n",
    "            node_input_dim=node_dim,\n",
    "            embedding_dim=emb_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_attention_layers=num_attention_layers,\n",
    "            num_heads=num_heads,\n",
    "        )\n",
    "        self.decoder = GraphDecoder(\n",
    "            emb_dim=emb_dim, num_heads=8, v_dim=emb_dim, k_dim=emb_dim\n",
    "        )\n",
    "\n",
    "        self.model = lambda x, mask, rollout: self.decoder(\n",
    "            x, mask, rollout=rollout\n",
    "        )  # remove encoding and make it do it once\n",
    "\n",
    "    def forward(self, env, rollout=False) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Forward method of the model\n",
    "        Args:\n",
    "            env (gym.Env): environment which the agent has to solve.\n",
    "            rollout (bool, optional): policy decision. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: accumulated loss and log probabilities.\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        state = torch.tensor(env.get_state(), dtype=torch.float, device=self.device)\n",
    "        acc_loss = torch.zeros(size=(state.shape[0],), device=self.device)\n",
    "        acc_log_prob = torch.zeros(size=(state.shape[0],), device=self.device)\n",
    "\n",
    "        emb = self.encoder(x=state[:, :, :2])\n",
    "\n",
    "        while not done:\n",
    "            actions, log_prob = self.decoder(\n",
    "                node_embs=emb, mask=state[:, :, 3], rollout=rollout\n",
    "            )\n",
    "\n",
    "            state, loss, done, _ = env.step(actions.cpu().numpy())\n",
    "\n",
    "            acc_loss += torch.tensor(loss, dtype=torch.float, device=self.device)\n",
    "            acc_log_prob += log_prob.squeeze().to(self.device)\n",
    "\n",
    "            state = torch.tensor(env.get_state(), dtype=torch.float, device=self.device)\n",
    "\n",
    "        self.decoder.reset()\n",
    "\n",
    "        return acc_loss, acc_log_prob  # shape (batch_size), shape (batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_dim: int = 2,\n",
    "        emb_dim: int = 128,\n",
    "        hidden_dim: int = 512,\n",
    "        num_attention_layers: int = 3,\n",
    "        num_heads: int = 8,\n",
    "        lr: float = 1e-4,\n",
    "        csv_path: str = \"loss_log.csv\",\n",
    "        seed=69,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The TSPAgent is used in companionship with the TSPEnv\n",
    "        to solve the traveling salesman problem.\n",
    "\n",
    "        Args:\n",
    "            node_dim (int): Input dimension of a regular graph node.\n",
    "            emb_dim (int): Size of a vector in the embedding space.\n",
    "            hidden_dim (int): Dimension of the hidden layers of the \n",
    "                ff-network layers within the graph-encoder.\n",
    "            num_attention_layers (int): Number of attention layers \n",
    "                for both the graph-encoder and -decoder.\n",
    "            num_heads (int): Number of attention heads in each \n",
    "                MultiHeadAttentionLayer for both the graph-encoder and -decoder.\n",
    "            lr (float): learning rate.\n",
    "            csv_path (string): file where the loss gets saved.\n",
    "            seed (int): the seed.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.csv_path = csv_path\n",
    "        self.model = TSPModel(\n",
    "            node_dim=node_dim,\n",
    "            emb_dim=emb_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_attention_layers=num_attention_layers,\n",
    "            num_heads=num_heads,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.target_model = TSPModel(\n",
    "            node_dim=node_dim,\n",
    "            emb_dim=emb_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_attention_layers=num_attention_layers,\n",
    "            num_heads=num_heads,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        env,\n",
    "        epochs: int = 100,\n",
    "        eval_epochs: int = 1,\n",
    "        check_point_dir: str = \"./check_points/\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the TSPAgent on an TSPEnvironment.\n",
    "\n",
    "        Args:\n",
    "            env: TSPEnv instance to train on\n",
    "            epochs (int, optional): Amount of epochs to train. Defaults to 100.\n",
    "            eval_epochs (int, optional): Amount of epochs to evaluate the current \n",
    "                model against the baseline. Defaults to 1.\n",
    "            check_point_dir (str, optional): Directiory that the checkpoints will\n",
    "                be stored in. Defaults to \"./check_points/\".\n",
    "        \"\"\"\n",
    "        logging.info(\"Start Training\")\n",
    "        with open(self.csv_path, \"w+\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Epoch\", \"Loss\", \"Cost\", \"Advantage\", \"Time\"])\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        for e in range(epochs):\n",
    "            self.model.train()\n",
    "\n",
    "            loss_m, loss_b, log_prob = self.step(env, (False, True))\n",
    "            advantage = (loss_m - loss_b) * -1\n",
    "            loss = (advantage * log_prob).mean()\n",
    "\n",
    "            # backpropagate\n",
    "            self.opt.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            self.opt.step()\n",
    "\n",
    "            # update model if better\n",
    "            self.baseline_update(env, eval_epochs)\n",
    "\n",
    "            logging.info(\n",
    "                f\"Epoch {e} finished - Loss: {loss}, Advantage: {advantage.mean()} Dist: {loss_m.mean()}\"\n",
    "            )\n",
    "\n",
    "            # log training data\n",
    "            with open(self.csv_path, \"a\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(\n",
    "                    [\n",
    "                        e,\n",
    "                        loss.item(),\n",
    "                        loss_m.mean().item(),\n",
    "                        advantage.mean().item(),\n",
    "                        time.time() - start_time,\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            self.save_model(episode=e, check_point_dir=check_point_dir)\n",
    "\n",
    "    def save_model(self, episode: int, check_point_dir: str) -> None:\n",
    "        \"\"\"\n",
    "        Saves the model parameters every 50 epochs.\n",
    "\n",
    "        Args:\n",
    "            episode (int): Current episode number\n",
    "            check_point_dir (str): Directory where the checkpoints\n",
    "                will be stored.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(check_point_dir):\n",
    "            os.makedirs(check_point_dir)\n",
    "\n",
    "        if episode % 50 == 0 and episode != 0:\n",
    "            torch.save(\n",
    "                self.model.state_dict(), check_point_dir + f\"model_epoch_{episode}.pt\",\n",
    "            )\n",
    "\n",
    "    def step(self, env, rollouts: Tuple[bool, bool]):\n",
    "        \"\"\"\n",
    "        Plays the environment to completion for\n",
    "        both the baseline and the current model.\n",
    "\n",
    "        Resets the environment beforehand.\n",
    "\n",
    "        Args:\n",
    "            env (gym.env): Environment to train on\n",
    "            rollouts (Tuple[bool, bool]): Each entry decides \n",
    "                if we sample the actions from the learned\n",
    "                distribution or act greedy. Indices are for\n",
    "                the current model (0) and the baseline (1).\n",
    "\n",
    "        Returns:\n",
    "            (Tuple[torch.tensor, torch.tensor, torch.tensor]): \n",
    "                Tuple of the loss of the current model, the loss\n",
    "                of the baseline and the log_probability for the \n",
    "                current model.\n",
    "        \"\"\"\n",
    "        env.reset()\n",
    "        env_baseline = deepcopy(env)\n",
    "\n",
    "        # Go through graph batch and get loss\n",
    "        loss, log_prob = self.model(env, rollouts[0])\n",
    "        with torch.no_grad():\n",
    "            loss_b, _ = self.target_model(env_baseline, rollouts[0])\n",
    "\n",
    "        return loss, loss_b, log_prob\n",
    "\n",
    "    def evaluate(self, env):\n",
    "        \"\"\"\n",
    "        Evalutes the current model on the given environment.\n",
    "\n",
    "        Args:\n",
    "            env (gym.env): TSPAgent (or inherited) environment\n",
    "                to evaluate\n",
    " \n",
    "        Returns:\n",
    "            torch.Tensor: Reward (e.g. -cost) of the current model.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss, _ = self.model(env, rollout=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def baseline_update(self, env, batch_steps: int = 3):\n",
    "        \"\"\"\n",
    "        Updates the baseline with the current model iff\n",
    "        it perform significantly better than the baseline.\n",
    "\n",
    "        Args:\n",
    "            env (gym.env): Env to step through\n",
    "            batch_steps (int, optional): How many games to play.\n",
    "        \"\"\"\n",
    "        logging.info(\"Update Baseline\")\n",
    "        self.model.eval()\n",
    "        self.target_model.eval()\n",
    "\n",
    "        current_model_cost = []\n",
    "        baseline_model_cost = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(batch_steps):\n",
    "                loss, loss_b, _ = self.step(env, [True, True])\n",
    "\n",
    "                current_model_cost.append(loss)\n",
    "                baseline_model_cost.append(loss_b)\n",
    "\n",
    "        current_model_cost = torch.cat(current_model_cost)\n",
    "        baseline_model_cost = torch.cat(baseline_model_cost)\n",
    "        advantage = ((current_model_cost - baseline_model_cost) * -1).mean()\n",
    "        _, p_value = stats.ttest_rel(\n",
    "            current_model_cost.tolist(), baseline_model_cost.tolist()\n",
    "        )\n",
    "\n",
    "        if advantage.item() <= 0 and p_value <= 0.05:\n",
    "            print(\"replacing baceline\")\n",
    "            self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, {'coordinates': array([0.39328656, 0.21067615]), 'num_chargers': array([3]), 'available_chargers': array([3]), 'trailers': {'Trailer A': {'destination_node': 2, 'start_time': 16, 'end_time': 17.5}}, 'trucks': {'Truck 0': {'battery_level': 1}}, 'node_color': 'black'}), (1, {'coordinates': array([0.83412584, 0.05875696]), 'num_chargers': array([8]), 'available_chargers': array([8]), 'trailers': {'Trailer B': {'destination_node': 0, 'start_time': 14, 'end_time': 15.5}}, 'trucks': {'Truck 1': {'battery_level': 1}}, 'node_color': 'black'}), (2, {'coordinates': array([0.86830811, 0.30618329]), 'num_chargers': array([1]), 'available_chargers': array([1]), 'trailers': {'Trailer C': {'destination_node': 1, 'start_time': 15, 'end_time': 17.0}}, 'trucks': None, 'node_color': 'black'}), (3, {'coordinates': array([0.50124365, 0.08149117]), 'num_chargers': array([2]), 'available_chargers': array([2]), 'trailers': None, 'trucks': None, 'node_color': 'black'})]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAAGFCAYAAABg2vAPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAz30lEQVR4nO3de3RU9b3//9fMJAQwCVeJQIIpN7kkOpBwkUtIFCtHkYuggKgEsDZWxR5OWzn1aDFH0FOP4pGljS03kRRclCKFgi4l3CIQAkKQohWRCBgCKUoSwJC57N8f+TI/Qm4TmJlksp+PtVzLyd7783lvcDmv7P3Z720xDMMQAAAwLWtDFwAAABoWYQAAAJMjDAAAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmF+LNTm63WwUFBYqIiJDFYvF3TQAAwAcMw1Bpaak6deokq7Xm3/+9CgMFBQWKiYnxWXEAACBwTpw4oejo6Bq3exUGIiIiPINFRkb6pjIAAOBXJSUliomJ8XyP18SrMHD51kBkZCRhAACAIFPXLX4WEAIAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJgcYQAAAJMjDAAAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJgcYQAAAJMjDAAAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJgcYQAAAJMjDAAAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJgcYQAAAJMjDAAAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJgcYQAAAJMjDAAAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJgcYQAAAJMjDAAAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJgcYQAAAJMjDAAAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJgcYQAAAJMjDAAAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJgcYQAAAJMjDAAAYHKEAQAATI4wAACAyYU0dAEAAP85ffq0tm/frn379unAgQP64YcfJElt2rSR3W5XQkKCkpKSFBUV1cCVoiERBgCgiTEMQ9u3b9dbb72lv/71r3K5XAoNDZXT6ZRhGJIki8WirKwsORwO2Ww23X///XryySeVlJQki8XSwGeAQOM2AQA0IadPn9b999+v5ORkrV27Vi6XS5LkcDg8QUCqCAwOh0OS5HK5tHbtWiUnJ+v+++/X6dOnG6R2NBzCAAA0EZs2bVKvXr20fv16SZLT6fT62Mv7rl+/Xr169dKmTZv8UiMaJ8IAADQBq1ev1n333afi4mLP1YBr4XK5VFxcrPvuu09/+ctffFghGjPCAAAEuaysLE2ZMkVut7vSrYBrZRiG3G63pkyZoqysLB9UiMaOMAAAQay4uFhTp06VYRg+CQKXXQ4EU6dOVXFxsc/GReNEGACAIDZ79mwVFRXJ7Xb7fGy3262ioiLNnj3b52OjcbEYXkTJkpIStWrVSsXFxYqMjAxEXQCAOhw9elTdu3f3+zwWi0VHjhxRt27d/D4XfMvb72+uDABAkMrIyJDNZvP7PFarVe+8847f50HD4coAAAQhp9Op9u3bB+x+fqtWrXT27NmAhA/4DlcGAKAJ+/LLLwO6sK+4uFhffvllwOZDYBEGACAI7du3zxRzIjAIAwAQhA4fPqzQ0NCAzRcaGqrDhw8HbD4EFmEAAILQxYsXTTEnAoMwAABBqCEW8rF4sOkiDABAEGrfvr1fGg3VxO12q3379gGbD4FFGACAINS/f//reiFRfblcLvXv3z9g8yGwCAMAEIQSEhICPidhoOkiDABAEIqKipLdbpfV6v//jVutVvXr109RUVF+nwsNgzAAAEFq1qxZAVk34Ha7NWvWLL/Pg4ZDGACAIDVp0iS1adNGFovFb3NYLBa1adNGkyZN8tscaHiEAQAIUi1bttTbb78tL14xc80Mw9Af/vAHtWjRwm9zoOERBgAgiE2aNEnjxo3zSw8Am82m8ePH68EHH/T52GhcCAMAEMQsFouWLFminj17+jQQ2Gw29ezZU4sXL/brbQg0DoQBAAhybdq00ZYtW9SrVy+fBAKbzaZevXppy5YtatOmjQ8qRGNHGACAJiAqKko7d+5UamqqJF3TI4eXj0lNTdXOnTt5lNBECAMA0ERERkZq0aJF+vDDD9W1a1dJUkhISJ3HXd6na9eu+uijj7Ro0SJFRkb6tVY0LoQBAGhi7r77bn311VfavHmzxowZU+sXe2RkpMaMGaPNmzfrq6++0k9/+tMAVorGgjAAwBTsdrvsdrv69OmjkJAQz+f6PD+fkZGhBQsWSJKWLVumiRMn+qy+rKwsWSwWrVix4prHmDVrlmJjY2WxWPSPf/xDd9xxh9asWaNz587pm2++0aRJkxQVFaXOnTurb9++OnbsmM6dO6c1a9bojjvu8CwU3LBhg3r16qXu3btrwoQJOn/+vGeOnJwc2e129ezZU3feeadOnTp13bUeOnSo0rYhQ4Z4/n7i4uJksVh08ODBKmPk5+dX+ru02+06evSoz2s1BcMLxcXFhiSjuLjYm90BoNE6duyY0a5du2q3ORwOr8dZunSpMWHChHrP73Q6q/35Qw89ZCQnJxvJycn1HvOybdu2GSdOnDBuvvlm4/PPP6+07Y033jDuv/9+49KlS4ZhGEZBQUG1Y5SWlhodOnQwvvjiC8MwDOPJJ5805syZYxiGYbjdbqNbt27Gli1bDMMwjFdffdWYPHmyz2u90urVq424uLhqt9X2d+nLWoOZt9/fXBkAYGqxsbGaN2+eUlJSNG3aNBUWFiolJUUJCQnq27evZs2a5WnqM3fuXP3qV7+qdpz33ntPgwYNUv/+/TVixAjPb7vLli3TqFGj9OijjyoxMVF79uypcuy5c+e0ceNGrVy5Uv/4xz8q/XZbH0lJSYqOjq5226uvvqr/+Z//UbNmzSRJHTt2rHa/TZs2KTExUb169ZIk/eIXv9DKlSslSXv37lVYWJiSk5MlST//+c/1wQcfyOFw+LTWKy1ZskQzZ86s9/i+rNUMCAMATO/48ePKyspSZmamWrdurfXr12vfvn06ePCgvvnmG61Zs6bW4z/99FOtWrVK27dv12effaaXXnpJU6dO9WzPzs7W888/r7179+r222+vcnxmZqZ++tOf6qabbtLUqVO1ZMmSGue65557tHfv3nqdX0lJiYqKirR27VoNHjxYgwcP1vvvv1/tvsePH9fNN9/s+RwbG6vvvvtObre7yraIiAhFRET47fL7d999p61bt+rhhx+ucZ+SkhINGDBA/fv3V3p6uue1zoGuNdjVvcwUAJq46dOne+6Xu91uPfvss8rOzpZhGDpz5ozsdnut6wPWrVunvLw8DRo0yPOzoqIilZeXS5KGDRumHj161Hj84sWL9fLLL0uSZs6cqbvvvlvp6enV9gzYuHFjvc/P4XCovLxcP/74o3bv3q3jx4/r9ttvV9++fRUXF1dl/9qaDF29zfBjK+Rly5Zp9OjRat++fbXbO3bsqJMnT6pDhw76/vvvNWnSJL322mv6zW9+E/Bagx1XBgCYXnh4uOffX3/9dZ09e1Y5OTk6ePCgHnroIZWVldV6vGEYmjFjhg4cOOD5p6CgwHNJ/srxr3bgwAF9/vnnevzxxxUbG6vRo0frX//6lz788EPfnJykdu3aKTw83PMbdpcuXTR06NBqrzB06dJF+fn5ns/5+fnq3LmzrFZrlW2lpaUqLS2t9pbD5UWAVwak+jAMQ0uXLq31FkFYWJg6dOggSWrbtq1mzJihHTt2VHsetdUKwgAAVPLDDz/opptuUvPmzXX69GmtXr26zmPuu+8+LV++XCdOnJBUcXXB20v5ixYt0n/8x3/o22+/VX5+vvLz8/Xaa69p8eLF13UeV5syZYonYPzwww/as2ePbr311ir7jRo1Srm5ufryyy8lSW+//bYmT54sSUpISFBZWZm2bt0qSXrnnXc0btw4hYaGVhln586dOnDggHJycq6p3m3btqm8vFx33XVXjfucOXPGswbg0qVL+utf/6p+/frVu1YQBgCgklmzZmnnzp2y2+2aMWOGRo4cWecxSUlJmj9/vsaOHavbbrtNcXFxNd6Tv1JZWZn+/Oc/V1pfIEmTJ0/WRx99pNOnT1c5prY1A08++aSio6N18uRJjRw5Ut27d/dsmz9/vjZt2qS4uDgNHz5c//mf/6n+/ftLkl544QVlZGRIqri3vmjRIo0bN07du3fXd999p9/+9reSKjoUrlixQs8884x69uypv//973rttdfqPM/61ipV3DqZPn16lU6KV9aanZ2tfv366bbbblP//v1100036bnnnvN5rWZgMby4iVJSUqJWrVqpuLiYrlQAAAQJb7+/uTIAAIDJEQYAADA5wgAAACZHGACABmSxWHTrrbd6+gfMnTtXy5YtU0ZGhqffftu2bRUdHe35vGXLFq/Ht9vt+vHHHyVVNBC6+j0A1+r9999XYmKibrnlFvXp00f33XefPv/8c888l02dOlU33XRTjZ0b0TjQdAgAGtjOnTur9CJIS0tTWlqaJCk1NVWJiYl66qmnqhzrcrmqbU502YEDB66rNqfTWeU1yEuXLtXLL7+sDz74QH369JEk7du3TwUFBYqPj6+0b2ZmpubOnVvpZUdofAgDANCIhIeHq0WLFjVuX7ZsmVatWqUOHTro8OHDWrhwoXbt2qWVK1fK6XQqNDRUCxcu9DT7sVgsKi0trRI2CgsLNWvWLOXn56usrEzjxo1Tenq6pIrf7H/2s5/pk08+UadOnZSZmVnp2N/97nfKyMjwBAGp4rn+y2688cbr/nNAYBEGAKAR8eZyenZ2tvbv3+9pcdy9e3fNnj1bkrR7927NnDmzztsB06ZN03PPPaekpCQ5nU6NHj1aa9eu1fjx4yX9/+9ruLql75kzZ3TixIlq37FwWW5ubp3ngMaFMAAAQebqdx3s379f8+bN09mzZxUSEqLDhw+rvLzc0w75ahcuXFBWVlalpkbnz5/3dB2UKr+vAU0fYQAAgsyVl/zLy8s1YcIEbd26VQkJCZ4mM7WFAbfbLYvFotzc3Brb89b0PoUOHTooOjpau3bt0j333HP9J4NGgacJACCIlZWVyeFwKCYmRpK0cOHCOo+JiIjQ8OHD9corr3h+VlBQoJMnT3o159y5czV79uxKVxJ27dqlTZs21bN6NBZcGQCAIBYZGan09HQNHDhQXbp00ZgxY7w6LjMzU7Nnz/as/g8PD1dGRoaio6PrPHbmzJlq0aKFpk6dqvPnzyskJETdunXzvIYZwYd3EwBAA6pptX9TcvnRwv/93/9t6FJMh3cTAEAQiIqK0tChQz1Nh5qaqVOnasWKFfwi2chxmwAAGlBhYWFDl+BXV/coQOPElQEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMLmQhi4AjUN5ebkOHTqkvLw8lZSUSJIiIyN12223KS4uTs2aNWvgCgEA/kIYMDGHw6F169bprbfeUnZ2tpxOpyTJZrNJklwulyQpJCREw4YN05NPPqmxY8cqNDS0wWoGAPgetwlMyDAMrVq1StHR0XrggQe0Y8cOTxCQKkLA5SAgSU6nUzt27NADDzyg6OhorVq1SoZhNETpAAA/IAyYTFFRke6//35NmTJFRUVFklTpi78ml/cpKirSlClTdP/993uOBwAEN8KAiXz77bcaOHCg1q9fL0nX9Nv95WPWr1+vQYMG6fjx4z6tEQAQeIQBkygsLNSIESN08uRJr64E1MXlcunEiRNKSkpSYWGhDyoEADQUwoAJGIahRx55RN99912ltQHXy+l06rvvvtOjjz7KGgIACGKEARNYsmSJPvnkE58GgcucTqc+/vhjLV261OdjAwACw2J48StdSUmJWrVqpeLiYkVGRgaiLvhIaWmpOnXqpPPnz/t1nvDwcJ06dUrh4eF+nQcA4D1vv7+5MtDEZWZm6sKFC36f58KFC8rMzPT7PAAA3yMMNGGGYejNN98M2HyBnAsA4DuEgSasoKBAX3zxRUAW9xmGocOHD6ugoMDvcwEAfIsw0ITt27fPFHMCAK4PYaAJ279/v0JCAvf6iZCQEO3fvz9g8wEAfIMw0IR9//33slgsAZvPYrHo+++/D9h8AADfIAw0YW632xRzAgCuD2GgCWuIZ/5vuOGGgM8JALg+hIEmLC4uTg6HI2DzORwOxcfHB2w+AIBvEAaasP79+5tiTgDA9SEMNGE9e/ZUu3btAjZfu3bt1LNnz4DNBwDwDcJAE2az2ZSWliabzRaQuZ544glZrfwnBQDBhv9zN3GPP/54QFb4u91uPf74436fBwDge4SBJq5Lly56+umn/fobu9Vq1axZsxQTE+O3OQAA/sMrjE3gwoUL6tu3r06ePCmXy+XTsW02m2JiYvSPf/xDLVu29OnYAIDrwyuM4XHDDTdo1apVstlsPu1IaLFYFBISopUrVxIEACCIEQZMYvDgwVq3bp1CQ0N9csvAarUqNDRU69at0+DBg31QIQCgoRAGTGTUqFHavHmzOnTocF1PGNhsNkVFRWnz5s26++67fVghAKAhEAZMZtiwYfryyy+VmpoqSfV6q+HlfVNTU/XFF19o2LBh/igRABBghAETatWqlRYtWqS8vDzNnDlTzZs3l1Rx6f/KKwY2m81zS6F58+Z67LHHlJeXp0WLFqlVq1YNUjsAwPd4mgAqLS3Vnj17tG/fPuXl5encuXOSpNatW+u2225TQkKCBg4cqIiIiIYtFABQL95+f3t/jRgNxmKxKD4+Xq+88oruuecezZ07V7GxsSorK1NGRoYk6fjx42rZsqXat28vSVqwYIFSUlK8Gn/48OHatWuX7rzzTsXGxmrDhg2Ki4vzSe1JSUk6deqUvvrqq0pPMsTGxio/P1+SlJKSory8PKWnp+upp57yybwAAO8RBoLEzp07q7ySOC0tTWlpaZIq7uMnJiZW+2XqcrlqXTB44MCB66rN6XRWu/bgyJEjOnLkiNq0aaPt27drxIgR1R6/ZcsWzxoGAEDgsWYgCIWHh6tFixY1bl+2bJlGjRqlRx99VImJidqzZ49ef/11DRgwQP369dPAgQOVk5Pj2d9isej8+fNVxiksLNSDDz6ogQMH6tZbb9ULL7zg2RYbG6t58+YpJSVF06ZNq7aOxYsX6+GHH9Zjjz2mxYsXV9p244031ve0AQB+wpWBIPSrX/2qzn2ys7O1f/9+9ejRQ5LUvXt3zZ49W5K0e/duzZw5U4cOHap1jGnTpum5555TUlKSnE6nRo8erbVr12r8+PGSKm5NZGVlVdvIyOl0avny5dq8ebPat2+v9PR0FRcXexYe5ubm1uucAQD+QxhoooYNG+YJApK0f/9+zZs3T2fPnlVISIgOHz6s8vJyNWvWrNrjL1y4oKysLJ0+fdrzs/Pnz+vLL7/0fJ4+fXqNHQ03btyom2++Wb1795YkjRw5UitXrvTc1gAANB6EgSbqyvUF5eXlmjBhgrZu3aqEhATP6tLawoDb7ZbFYlFubq5CQ0PrnONqixcv1pEjRxQbGytJ+vHHH/Xtt98SBgCgEWLNgAmUlZXJ4XB43iq4cOHCOo+JiIjQ8OHD9corr3h+VlBQoJMnT9Z5bGFhoTZv3qyvv/5a+fn5ys/PV0FBgU6cOKGDBw9e+4kAAPyCMGACkZGRSk9P18CBA5WUlKSwsDCvjsvMzNQXX3yh+Ph4xcfHa8KECTp79mydx7377ru6++671bp1a8/PbDabpkyZokWLFl3raQAA/ISmQ0HAYrGotLS01svywa62RyMBANeGVxg3IVFRURo6dKg2btzY0KX4RUpKirZt26YbbrihoUsBAFNiAWEQKCwsbOgS/GrLli0NXQIAmBpXBq5gt9tlt9vVp08fhYSEeD5PmjTJ6zEyMjK0YMECSRXNfyZOnHjddeXn51eqp3fv3nrppZeuaaxZs2YpNjZWFoulxj4D7777riwWizZs2FDjOBs2bFCvXr3UvXt3TZgwoVLTopycHNntdvXs2VN33nmnTp065fNak5OT1bVrV8+fyeU/86td/Wdnt9t19OhRn9cKAEHN8EJxcbEhySguLvZm96B37Ngxo127dtVuczgcXo+zdOlSY8KECfWe3+l01lpPSUmJERUVZRw6dKjeY2/bts04ceKEcfPNNxuff/55le0nTpwwbr/9dmPw4MHG+vXrqx2jtLTU6NChg/HFF18YhmEYTz75pDFnzhzDMAzD7XYb3bp1M7Zs2WIYhmG8+uqrxuTJk+tdZ121jhgxosb6rlTb36UvawWAxsjb72+uDHjh6ta7hYWFSklJUUJCgvr27atZs2bJ+H/rMOfOnVtjh8D33ntPgwYNUv/+/TVixAjPb7vVtQ+uTWlpqQzDuKbFnElJSYqOjq5x++OPP64FCxbU+sTBpk2blJiYqF69ekmSfvGLX2jlypWSpL179yosLEzJycmSpJ///Of64IMP5HA4fF7r9fJlrQAQzAgDXrrcejczM1OtW7fW+vXrtW/fPh08eFDffPON1qxZU+vxn376qVatWqXt27frs88+00svvaSpU6d6tmdnZ+v555/X3r17dfvtt1c5/ty5c7Lb7YqPj9dPfvITPf74456+AVe75557tHfv3nqf4x/+8Af17dtXgwYNqnW/48eP6+abb/Z8jo2N1XfffSe3211lW0REhCIiIvxy+f3Xv/614uPjNWnSJH3zzTc17ldSUqIBAwaof//+Sk9Pl8vlqvY8/FkrADRmLCD00pWtd91ut5599lllZ2fLMAydOXNGdru91vUB69atU15eXqUv2qKiIpWXl0uq2j74aq1bt/a8XfD777/XnXfeqQEDBmjMmDFV9r2Wpw6OHTumP/3pT/r000+92r+mNsTVbTPqfnq13t577z3FxMTIMAy99dZbGj16tA4fPlxlv44dO+rkyZPq0KGDvv/+e02aNEmvvfaafvOb3wSsVgBo7Lgy4KUrn/F//fXXdfbsWeXk5OjgwYN66KGHVFZWVuvxhmFoxowZOnDggOefgoICTzvg+vQQaNu2re666y599NFH13Yy1di1a5cKCgrUu3dvxcbGel5m9Kc//anKvl26dFF+fr7nc35+vjp37iyr1VplW2lpqUpLS9WxY8cq4wwZMkR2u73OKxHVuXxVxGKx6KmnntI333xTbUOksLAwdejQQVLFn9uMGTO0Y8eOas+jtloBoCkjDFyDH374QTfddJOaN2+u06dPa/Xq1XUec99992n58uU6ceKEpIqrC9dyKV+SLl26pE8//VS33HLLNR1fnYceekiFhYWe9sGDBw/W4sWL9bOf/azKvqNGjVJubq7npUVvv/22Jk+eLElKSEhQWVmZtm7dKkl65513NG7cuGrfb7Bz504dOHCg0uuUveF0Oiu9QGnNmjWKiopSu3btqux75swZzxqAS5cu6a9//av69etX71oBoCnjNsE1mDVrlh544AHZ7XZ17txZI0eOrPOYpKQkzZ8/X2PHjpXL5ZLD4dC9996rxMREr+a8vGZAqvhSS0lJ0RNPPFHtvvfcc4/S09OrHfvJJ5/UunXrVFhYqJEjRyo8PFxff/11nfO/8MIL6tSpk9LS0hQREaFFixZp3Lhxcjqdio+P17vvvitJslqtWrFihdLS0vTjjz+qc+fOWrFihVfn6G2tly5d0r333qtLly7JarWqffv2+tvf/lZtrdnZ2XrhhRdks9nkdDp1xx136LnnnvN5rQAQzGhHDABAE0U7YgAA4BXCAAAAJkcYCCIWi0W33nqr59HBuXPnatmyZcrIyPC02m3btq2io6M9n33R97+2RkpXe+mll9StWzd169ZNzz//vOfnW7duVWpqqiTp6NGjstvtatasWY0tkQEAgcMCwiCzc+fOKo8hpqWlKS0tTVLtrwJ2uVyy2Wx+q2379u1auXKlDh48qJCQEA0dOlTDhg3T3XffXWm/bt266cCBA4qNjfVbLQAA73FlIIiFh4erRYsWNW6vrs1xcnJypRcQTZw4UcuWLZMkFRcX67HHHlN8fLxuu+02zZgxo8qYhw8fVnx8vDZt2lRl2/vvv6/U1FTdcMMNCgsL04wZMzxtips1a6ZWrVpd5xkDAPyBKwNBzJtL99nZ2dq/f3+t3Q0v++Uvf6nw8HDl5eXJarWqqKio0vasrCw9/fTTyszM9DzmeKXjx49rxIgRns+xsbH6y1/+IqmiwdCQIUPqrAEAEHiEgSaurjbHV9qwYYP27dsnq7XigtGNN97o2fbxxx9r48aN+uijj2p8J4JUub0vrX0BIDhwm6CJu3p9QUhIiOdFPZLqbKN8WY8ePWQYRq1vVLy6ve+3336rLl261K9gAEDAEQZMplu3bp72v8eOHVN2drZn25gxY/Tqq6/K7XZLUqXbBLGxsdq8ebNefPFFLV++vNqxH3jgAb377ru6cOGCLl26pCVLlnjaFAMAGi9uE5jMs88+q0mTJumjjz7SLbfcUuklQQsWLNC///u/Ky4uTs2aNdOAAQMqvaioU6dOysrK0qhRo3T+/Hn94he/qDR2cnKyHnzwQcXHx0uSJk+erFGjRgXmxAAA14x2xEHEYrGotLS0Xm84bMxiY2O1YcMGxcXFNXQpANAk0Y64CYqKitLQoUM9TYeC1eWmQw6HgzcEAkAjwG2CIFJYWNjQJfjE5aZDAIDGgSsDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTC2noAgB/O378uNauXau9e/dq9+7dKiwslNPpVLNmzdSjRw8NGjRIgwYN0vjx4xUREdHQ5QJAwFkMwzDq2qmkpEStWrVScXGxIiMjA1EXcN22bt2q1157TX//+99lsVhktVrldDqr7BcaGiqHw6GWLVtq+vTpmj17trp27doAFQOAb3n7/c1tAjQ5586d0/Tp05WSkqIPP/xQhmHI7XZXGwQkyeFwSJIuXryod955R3369NGbb74pt9sdyLIBoMEQBtCk5OXlqXfv3nrvvfckqcYAUBOn06lLly7pmWee0Z133qni4mJ/lAkAjQphAE3GZ599puHDh6uoqEgul+u6x9uxY4eSk5MJBACaPMIAmoSCggLdddddunjxok+CgCS5XC59/vnnGjduHLcMADRphAEEPcMw9Nhjj6m4uNhnQeAyl8ulrVu36u233/bpuADQmBAGEPQyMzO1adMmnweBK/36179Wfn6+38YHgIZEGEBQMwxDL774oiwWi1/ncTqdeuONN/w6BwA0FMIAgtqWLVv09ddfy4t2GdfF6XRq8eLFunDhgl/nAYCGQBhAUHv33XcVEhKYRprnz5/XunXrAjIXAAQSYQBBLTs7u969BK5VaGio9uzZE5C5ACCQCAMIWqWlpTp27FjA5nM4HMrJyQnYfAAQKIQBBK1vvvnG72sFrvbVV18FdD4ACATCAIJWWVlZwOcsLy8P+JwA4G+EAQSt0NBQU8wJAP5GGEDQ6ty5c8Dn7NixY8DnBAB/C8wzWYAfREVFqUOHDjpz5kxA5gsJCdHgwYMDMhfMzTAM5ebmauPGjcrNzVVubq6Ki4tlGIaaN2+uuLg4DRw4UElJSRo9erSaNWvW0CUjyBEGENQGDx6sDRs2BORFQi6XS4mJiX6fB+blcrm0YsUKLViwQHl5eQoJCZHL5aq0UNbhcGjXrl3au3ev/u///k/t27fXE088oWeeeUbt2rVrwOoRzLhNgKD24IMPBuyNglarVePGjQvIXDCfI0eOaNiwYUpNTdXnn38uqaLzZU1PzDgcDknSv/71L82fP1+33HILTbFwzQgDCGoTJ05U69at/T5PSEiIxo8fz5oB+MX777+vuLg47d27V5LqHXBdLpd++OEHjRs3To8//rhfX9qFpokwgKAWFhamX/7ylwF5UdHs2bP9OgfM6b333tOUKVPkcDiuq5vm5QCxaNEiPfzwwwQC1AthAEFvzpw56tGjh2w2m1/Gt1qteuKJJ3T77bf7ZXyYV1ZWllJTU2UYhs8aaBmGoffff19z5szxyXgwB4vhxX+BJSUlatWqlYqLixUZGRmIuoB6yc3N1eDBg32+fsBms6lTp046fPiwwsPDfTo2zK20tFS9e/fWqVOn/LLuxWKxaNu2bRo+fLjPx0bw8Pb7mysDaBIGDBigZcuW+XRMm82miIgIffjhhwQB+NycOXP8FgSkiitajz76aIN06kTwIQygyXjkkUe0fPlyWa3W675lYLPZ1LZtW+3YsUN9+vTxUYVAhTNnzuiPf/yjX5+Ecblcys/P1+rVq/02B5oOwgCalEceeUQ5OTnq3r37NS0qvHzM2LFjdejQIcXFxfm6RECLFy8OyCOxVqtVCxcu9Ps8CH6EATQ5iYmJysvL04svvqi2bdtKUp1XCkJCKvpv9e7dW6tXr9aaNWvUoUMHv9cKcwpUGHC73crNzeVtm6gTYQBNUlhYmJ5//nmdOnVKK1eu1NixY9WpU6cq+1mtVvXu3VvTp0/Xp59+qkOHDmnixIkNUDHM4ty5czp69GhA59yzZ09A50PwoR0xmrRmzZpp8uTJmjx5sqSKbm2FhYUqLy9X8+bN9ZOf/EQtWrRo4CphJp999llA5wsNDdXevXv18MMPB3ReBBfCAEylffv2at++fUOXARML9FUBh8MR8DkRfLhNAAABdOnSJb93zLzaxYsXAzofgg9hAAACKDQ01GfdBr0VFhYW0PkQfAgDABBAnTt3Duh8oaGhio6ODuicCD6EAQAIoISEhIDO53Q6Az4ngg9hAAACqGPHjgHtYWEYhhITEwM2H4ITYQAAAmzq1KmeRlf+Fhsbq379+gVkLgQvwgAABFhaWpqcTqff57FarXr66adltfK/etSO/0IAIMB69uyp0aNH+/XqgMViUXh4uFJTU/02B5oOwgAANIC3335bzZo189v4hmHorbfe8ryfA6gNYQAAGkBMTIzefPNNv4xts9k0evRoTZ061S/jo+khDABAA5kxY4Z+9atf+XRMm82muLg4ZWZmBrzTIYIXYQAAGojFYtHvf/97zZkzR5Kue6GfxWJRQkKCtmzZosjISF+UCJMgDABAA7JYLHr55Zf1wQcfqG3btrLZbPUew2azyWq16r/+67+0Y8cOtWnTxg+VoikjDABAIzB27Fj985//1BNPPKEWLVrIYrHUeaXAZrPJYrFo5MiRys3NVXp6ul8XJaLpshhevDGjpKRErVq1UnFxMZeeAMDPSktLlZmZqb///e/KyclRUVFRpe0tWrRQv379lJycrJkzZ6pr164NVCkaO2+/vwPTAgsAmhiLxaL4+Hi98soruueeezR37lzFxsaqrKxMGRkZkqTjx4+rZcuWat++vSRpwYIFSklJqXPsiIgIZWRkaNeuXWrRooViYmL01ltvqWvXrgoPD1dMTMw13U5ITU3VJ598ovbt28vtdqt169b64x//qF69ekmq6FaYn58vSUpJSVFeXp7S09P11FNP1XsuBBfCAABco507dyo8PLzSz9LS0pSWliap4ss3MTGx2i9Tl8tV6xf6gQMHPP9us9nUtWtXxcXFeV2b0+mstqnRnDlzPPX8/ve/1/PPP6/Vq1dX2W/Lli00LDIR1gwAgA+Eh4erRYsWNW5ftmyZRo0apUcffVSJiYnas2ePXn/9dQ0YMED9+vXTwIEDlZOT49nfYrHo/PnzVcYpLCzUgw8+qIEDB+rWW2/VCy+84NkWGxurefPmKSUlRdOmTau1XsMwdO7cuUqLDW+88cb6nDKaEK4MAIAPeNMvIDs7W/v371ePHj0kSd27d9fs2bMlSbt379bMmTN16NChWseYNm2annvuOSUlJcnpdGr06NFau3atxo8fL6ni1kRWVlaNPQZeeeUVLVq0SEVFRbLZbNq+fbtnW25urlfniqaHMAAAATJs2DBPEJCk/fv3a968eTp79qxCQkJ0+PBhlZeX1/hEwIULF5SVlaXTp097fnb+/Hl9+eWXns/Tp0+vtdnQlbcJlixZookTJ2rv3r3Xe2oIcoQBAAiQK9cXlJeXa8KECdq6dasSEhI8q75rCwNut1sWi0W5ubkKDQ2tc466TJ48WTNnzlRRURG3CEyONQMA0ADKysrkcDgUExMjSVq4cGGdx0RERGj48OF65ZVXPD8rKCjQyZMnr6mGzZs3q127dmrXrt01HY+mgysDANAAIiMjlZ6eroEDB6pLly4aM2aMV8dlZmZq9uzZio+Pl1RxJSAjI0PR0dFeHX95zYBhGAoLC9OaNWuuuw0ygh9NhwDgGlgsFpWWltbrsnywqe3RSAQHb7+/iYMAcA2ioqI0dOhQbdy4saFL8YuUlBRt27ZNN9xwQ0OXggDgNgEAXIPCwsKGLsGvtmzZ0tAlIIC4MgCgUbPb7bLb7erTp49CQkI8nydNmuT1GBkZGVqwYIGkiuY/EydO9EltZ86c0fTp09W1a1fFx8crPj5e8+fPr/c48+fP1y233CKr1aoNGzZU2paamqro6GjPef/617+ucZycnBzZ7Xb17NlTd955p06dOuXZduTIEQ0ZMkQ9e/bUwIEDdfjw4XrXWVetEydO9NRpt9tltVr1t7/9rcFqRT0YXiguLjYkGcXFxd7sDgA+d+zYMaNdu3bVbnM4HF6Ps3TpUmPChAn1nt/pdFb6fPHiRaNXr17G7373O8+28+fPG2+88Ua9x969e7fx9ddfGyNGjDDWr19fadu0adOMhQsX1jmG2+02unXrZmzZssUwDMN49dVXjcmTJ3u2p6SkGEuXLjUMwzBWr15tDB48uN511lXrlXJzc4127doZZWVlDVYrvP/+5soAgKB0devdwsJCpaSkKCEhQX379tWsWbNk/L/10XPnzq2xQ+B7772nQYMGqX///hoxYoSnA2B17YOv9Oc//1kRERGaO3eu5x0DN9xwg5555pl6n8ugQYPUrVu3eh93pb179yosLEzJycmSpJ///Of64IMP5HA4dObMGX322Wd6+OGHJUkTJkzQsWPHPC8l8ketS5Ys0cMPP6ywsLAGqxXeY80AgKB1ZevdsrIyrV+/XuHh4XK5XBo7dqzWrFlT6y2BTz/9VKtWrdL27dsVFhamHTt2aOrUqcrLy5NUtX3wlfbt26fbb7/d61ofe+wxjRkzxutHCK/0+uuv649//KO6dOmil156SXa7vco+x48f18033+z5HBERoYiICJ06dUpFRUXq1KmT58VFFotFXbp00fHjxxUbG1vveupSVlamlStXVmp13FhrRQXCAICgdWXrXbfbrWeffVbZ2dkyDENnzpyR3W6vNQysW7dOeXl5GjRokOdnRUVFKi8vl1S1ffD1WLRo0TUdN2/ePHXs2FFWq1Vr167Vv/3bv+nIkSPVPtJ4dRti44onx2vb5mtr1qxRjx49PL0QqtNYakUFbhMACFpXfiG+/vrrOnv2rHJycnTw4EE99NBDKisrq/V4wzA0Y8YMHThwwPNPQUGBpx1wbT0EEhIStHv3bt+cSC06d+7saQo0fvx4RUZG6p///GeV/bp06VLpUnppaalKS0vVsWNHxcTE6OTJk3I6nZIqzvvEiRPq0qVLlXGuXAR49uzZa6p58eLFmjlzZo3bfVUrfIcwAKBJ+OGHH3TTTTepefPmOn36tFavXl3nMffdd5+WL1+uEydOSKq4uuDtS3umTJmic+fO6b//+7/lcrkkSRcvXqzUKtgXrmw1vHv3bp09e1bdu3evsl9CQoLKysq0detWSdI777yjcePGKTQ0VB06dFC/fv20YsUKSRW/ucfGxlZ72f0vf/mLJxhdS5viY8eOac+ePZoyZUqN+/iqVvgOtwkANAmzZs3SAw88ILvdrs6dO2vkyJF1HpOUlKT58+dr7Nixcrlccjgcuvfee5WYmFjnsS1bttS2bds0Z84cde/eXeHh4bJYLHrooYeq3b+2NQMvv/yy3nrrLRUVFSk1NVXNmzfX/v37deONNyo1NVWnT5+WzWZTixYttHr1arVq1UpSxSOTBQUFSk9Pl9Vq1YoVK5SWlqYff/xRnTt39nyhShVfuKmpqZo/f74iIyP17rvv1nmO1amtVqli4eCECROqdLtriFrhPdoRAwDQRNGOGAAAeIUwAACAyREGAOA6WSwW3XrrrZ6XFs2dO1fLli1TRkaGZ2V+27ZtK7UV9kXv/9qaKV0pNzdXQ4YMUcuWLas8arl161alpqZKko4ePSq73a5mzZp5mi/BHFhACAA+sHPnziqPIqalpSktLU1S7a8Ddrlcni6G/tCxY0e98cYb2r9/vz7++OMa9+vWrZsOHDjAyn0T4soAAPhYeHi4WrRoUeP26lodJycnV3rxz8SJE7Vs2TJJUnFxsR577DHFx8frtttu04wZM6qMefjwYcXHx2vTpk1VtkVHR2vgwIHVtgZu1qyZ5+kEmBdXBgDAx7y5dF9bq+Or/fKXv1R4eLjy8vJktVpVVFRUaXtWVpaefvppZWZmVtuquDZDhgzRkCFD6nUMmh7CAAA0gPq0Ot6wYYP27dvn6UR4+Zl+Sfr444+1ceNGffTRR4qJifFLrWj6uE0AAA3g6vUFISEhnk6GkupspXxZjx49ZBhGlbcqAvVBGACARqBbt27KycmRVNHSNzs727NtzJgxevXVV+V2uyWp0m2C2NhYbd68WS+++KKWL18e2KLRZBAGAKARePbZZ/Xxxx8rISFBzz33XKU3KS5YsEAXL15UXFyc7Ha7fvvb31Y6tlOnTsrKytKbb76pt99+u8rYR48eVXR0tGbPnq2NGzcqOjq62v1gXrQjBoDrZLFYVFpaWutbDoNJbGysNmzYoLi4uIYuBdeJdsQAECBRUVEaOnSop+lQsLrcdMjhcCg0NLShy0EA8TQBAFynwsLChi7BJy43HYL5cGUAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMkRBgAAMDnCAAAAJkcYAADA5AgDAACYHGEAAACTIwwAAGByhAEAAEyOMAAAgMmFeLOTYRiSpJKSEr8WAwAAfOfy9/bl7/GaeBUGSktLJUkxMTHXWRYAAAi00tJStWrVqsbtFqOuuCDJ7XaroKBAERERslgsPi0QAAD4h2EYKi0tVadOnWS11rwywKswAAAAmi4WEAIAYHKEAQAATI4wAACAyREGAAAwOcIAAAAmRxgAAMDkCAMAAJjc/wcLxUsZnWXPewAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "G = EVRPGraph(num_nodes=4, num_trailers=3, num_trucks=2, plot_attributes=True)\n",
    "# add edges that where visited\n",
    "edges = [\n",
    "    (0, 3, 'Truck 1', 'Trailer B', 1),\n",
    "    (0, 3, 'Truck 2', None, 2),\n",
    "    (3, 2, 'Truck 1', 'Trailer A', 3),\n",
    "    (3, 2, 'Truck 2', 'Trailer C', 4),\n",
    "]\n",
    "\n",
    "print(G._nodes)\n",
    "G.visit_edge(edges)\n",
    "G.draw(ax=ax, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EVRPEnv.__init__() got an unexpected keyword argument 'num_trucks'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[25], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Init the environment\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mEVRPEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_nodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_trucks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_trailers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: EVRPEnv.__init__() got an unexpected keyword argument 'num_trucks'"
     ]
    }
   ],
   "source": [
    "# Init the environment\n",
    "env = EVRPEnv(num_nodes=20, batch_size=512)\n",
    "\n",
    "# Init the agent\n",
    "# agent = EVRPAgent(num_trucks=2, num_trailers=3)\n",
    "\n",
    "# Start training\n",
    "# agent.train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RL_Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "371fc9d26df76568c9d5bd269b799a646408d11c633a4ed335a6092dbc5b8a96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
